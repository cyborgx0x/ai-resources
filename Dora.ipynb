{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRG9cX76LxYO2E1SDKM78d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cyborgx0x/ai-resources/blob/main/Dora.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4hQRAxKVGNU",
        "outputId": "88ce440a-c786-4695-a63a-6cbe8284704c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([64, 20, 512])\n",
            "Output shape: torch.Size([64, 20, 512])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Transformer Encoder Layer\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dim_feedforward, dropout=0.1):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, nhead)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, dim_feedforward),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(dim_feedforward, d_model)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        src2 = self.self_attn(src, src, src)[0]\n",
        "        src = src + self.dropout(src2)\n",
        "        src = self.norm1(src)\n",
        "        src2 = self.feed_forward(src)\n",
        "        src = src + self.dropout(src2)\n",
        "        src = self.norm2(src)\n",
        "        return src\n",
        "\n",
        "# Transformer Encoder\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, nhead, dim_feedforward, dropout=0.1):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        encoder_layers = [TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout) for _ in range(num_layers)]\n",
        "        self.encoder = nn.Sequential(*encoder_layers)\n",
        "\n",
        "    def forward(self, src):\n",
        "        return self.encoder(src)\n",
        "\n",
        "# Example usage\n",
        "d_model = 512\n",
        "nhead = 8\n",
        "dim_feedforward = 2048\n",
        "num_layers = 6\n",
        "dropout = 0.1\n",
        "\n",
        "# Create a Transformer Encoder\n",
        "encoder = TransformerEncoder(num_layers, d_model, nhead, dim_feedforward, dropout)\n",
        "\n",
        "# Input data (batch_size, sequence_length, d_model)\n",
        "input_data = torch.randn(64, 20, d_model)\n",
        "\n",
        "# Forward pass through the Transformer Encoder\n",
        "output = encoder(input_data)\n",
        "\n",
        "print(\"Input shape:\", input_data.shape)\n",
        "print(\"Output shape:\", output.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vatq1MsTWcUe",
        "outputId": "297d9dd6-37aa-4cb1-9683-5308c9366671"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.0139,  0.8545, -0.7068,  ...,  0.6641, -0.3143, -0.2923],\n",
              "         [ 0.2339, -0.4133, -0.9518,  ...,  0.5721,  1.2003, -1.3061],\n",
              "         [-1.1030,  1.9347,  2.2691,  ..., -0.0179, -0.8216, -0.7187],\n",
              "         ...,\n",
              "         [-0.7892, -1.2245,  1.0740,  ...,  0.1568,  0.7171, -0.2822],\n",
              "         [-1.7963, -0.5028,  0.3921,  ...,  0.7665, -0.2996, -1.7582],\n",
              "         [-2.4838, -0.9078,  0.8984,  ..., -1.2333,  0.5882,  0.1788]],\n",
              "\n",
              "        [[-2.4267,  0.2237, -0.4580,  ...,  0.8302, -1.1847, -0.4490],\n",
              "         [-0.0203, -1.4386, -0.6680,  ...,  0.1567, -0.7015, -0.1183],\n",
              "         [-1.3083,  0.8586,  0.2094,  ...,  0.6625, -0.8445, -1.1805],\n",
              "         ...,\n",
              "         [ 0.7975, -0.1029, -0.3853,  ...,  0.7752,  1.2238,  0.4952],\n",
              "         [ 1.3483, -0.5947, -0.5905,  ...,  0.3817,  1.5105,  0.6956],\n",
              "         [-2.2890,  1.6490,  0.2945,  ..., -0.1354,  1.4039,  0.3567]],\n",
              "\n",
              "        [[-0.3118, -0.3506, -1.3435,  ..., -1.2020, -0.1497,  0.0778],\n",
              "         [-1.9191, -0.1921, -1.1249,  ...,  0.8903, -0.3682, -0.5117],\n",
              "         [-0.8616,  0.4603,  1.0069,  ..., -0.9742, -0.8089, -0.9158],\n",
              "         ...,\n",
              "         [-0.6141,  0.5627,  1.4019,  ..., -0.3376,  1.0576, -0.3038],\n",
              "         [-1.3011,  0.9136, -0.3665,  ...,  0.6496,  1.0668, -0.6154],\n",
              "         [ 0.1266,  0.3561,  0.3234,  ..., -1.7826, -0.2310, -1.5588]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[-0.9487,  0.4759,  0.3614,  ...,  0.3516, -0.1599, -0.5927],\n",
              "         [ 0.1862, -2.9515, -0.2027,  ..., -0.0168,  0.6245, -1.0068],\n",
              "         [ 0.5132, -0.0622, -0.5255,  ...,  0.0522, -0.9151, -1.2483],\n",
              "         ...,\n",
              "         [-0.8421,  0.1147,  0.5284,  ...,  0.4412,  0.5380, -2.4524],\n",
              "         [ 0.0551, -1.5376, -0.5339,  ..., -1.0521,  0.3053,  0.5517],\n",
              "         [-2.8417, -0.5563,  0.5689,  ..., -0.2785,  1.0087,  0.3371]],\n",
              "\n",
              "        [[-0.6693, -0.4958, -0.6071,  ..., -0.8571,  0.3129, -2.0195],\n",
              "         [-1.4983,  1.0580, -0.3875,  ..., -0.7327,  0.0305, -1.1216],\n",
              "         [ 0.3484,  0.5497, -0.7995,  ..., -0.2349, -0.5801,  0.0864],\n",
              "         ...,\n",
              "         [-0.4982, -0.1415,  0.8365,  ...,  0.6054,  0.5049, -0.6228],\n",
              "         [ 0.4271,  2.5547, -1.2140,  ...,  0.4585, -0.5502,  0.0800],\n",
              "         [-0.1895,  0.3426,  0.6293,  ..., -0.8000,  1.5614, -1.5142]],\n",
              "\n",
              "        [[ 0.3842, -0.1360,  0.6205,  ...,  1.1356,  0.4392,  0.4552],\n",
              "         [-0.1384, -1.9763,  0.4424,  ...,  2.2982, -0.6158, -0.2899],\n",
              "         [-0.2430, -0.0619, -1.2656,  ...,  0.2821,  1.5509, -0.3091],\n",
              "         ...,\n",
              "         [ 0.4519,  0.6976, -1.8722,  ...,  0.7813,  0.3667,  0.4554],\n",
              "         [ 0.4192,  0.6043,  0.9700,  ..., -2.2429,  0.1832, -2.3988],\n",
              "         [-0.8470, -0.7740, -0.8497,  ...,  0.1865,  0.4498,  0.2096]]],\n",
              "       grad_fn=<NativeLayerNormBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    }
  ]
}